{"cells":[{"cell_type":"code","execution_count":null,"id":"84f767b8-79d4-44e6-8ed4-3333eabe81ad","metadata":{"language":"python"},"outputs":[],"source":"!pip install numpy \n!pip install pandas\n!pip install matplotlib \n!pip install scikit-learn \n!pip install tensorflow \n!pip install seaborn"},{"cell_type":"code","execution_count":null,"id":"1b0ddbb2-cd7b-4a46-b1af-6d749baebdf1","metadata":{"language":"python"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras import regularizers, constraints, initializers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import wasserstein_distance"},{"cell_type":"code","execution_count":null,"id":"817454bd-9ddf-4188-b285-908bd0361f51","metadata":{"language":"python"},"outputs":[],"source":"github_url = 'https://raw.githubusercontent.com/aniruddhachoudhury/Red-Wine-Quality/master/winequality-red.csv'\nwine_data = pd.read_csv(github_url)\n\n# Check for missing values in the dataset\nmissing_values = wine_data.isnull().sum()\n\n# Print missing values if any\nprint(\"Missing values in each column:\\n\", missing_values)\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Select features for scaling\nfeatures = wine_data.columns #[:-1]\n\n# Fit and transform the data\nwine_data_scaled = wine_data.copy()\nwine_data_scaled[features] = scaler.fit_transform(wine_data[features])\n\n# Display the first few rows of the scaled dataset\nprint(wine_data_scaled.head())"},{"cell_type":"code","execution_count":null,"id":"b2e11b0b-34d7-4cd2-a531-9dd3cea89dc3","metadata":{"language":"python"},"outputs":[],"source":"def build_generator():\n    model = models.Sequential()\n    model.add(layers.Dense(512, input_dim=len(features), kernel_initializer='he_normal'))  # Increase the number of neurons\n    model.add(layers.ReLU()) # Use the ReLU activation function\n    model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(1024, kernel_initializer='he_normal'))  # Add layers and neurons\n    model.add(layers.ReLU())\n    model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(2048, kernel_initializer='he_normal'))  # Further increase in complexity\n    model.add(layers.ReLU())\n    model.add(layers.BatchNormalization(momentum=0.8))\n    model.add(layers.Dense(len(features), activation='linear'))  # Keep the last layer intact\n    return model\n\ndef build_discriminator():\n    model = models.Sequential()\n    model.add(layers.Dense(1024, input_dim=len(features), kernel_initializer='he_normal'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))  # Add Dropout\n    model.add(layers.Dense(1024, kernel_initializer='he_normal'))  # Increase the number of neurons\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Dense(512, kernel_initializer='he_normal'))   # Add more layers\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model"},{"cell_type":"code","execution_count":null,"id":"592f984e-bdb3-4fed-9747-5a173c9c1fad","metadata":{"language":"python"},"outputs":[],"source":"features = wine_data.columns \ngenerator = build_generator()\ndiscriminator = build_discriminator()\n\n# Define file paths where the weights will be saved\ngenerator_checkpoint_path =  'generator_weights.h5'\ndiscriminator_checkpoint_path = 'discriminator_weights.h5'\n\n# Create ModelCheckpoint callbacks\ngenerator_checkpoint = ModelCheckpoint(generator_checkpoint_path, save_weights_only=True, save_best_only=True, monitor='g_loss', mode='min')\ndiscriminator_checkpoint = ModelCheckpoint(discriminator_checkpoint_path, save_weights_only=True, save_best_only=True, monitor='d_loss', mode='min')\n\ndiscriminator.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\ngan = models.Sequential([generator, discriminator])\ndiscriminator.trainable = False\ngan.compile(loss='binary_crossentropy', optimizer='adam')"},{"cell_type":"code","execution_count":null,"id":"552b4b08-77b7-4ee0-82c8-fb5279d6f2ee","metadata":{"language":"python"},"outputs":[],"source":"def train_gan(gan, dataset, epochs=1000, batch_size=64, verbose=True ):  # Increase training cycles and batch sizes\n    valid = np.ones((batch_size, 1)) * 0.9  # Soft labels for real data\n    fake = np.zeros((batch_size, 1)) + 0.1  # Soft labels for fake data\n\n    checkpoint_g = ModelCheckpoint('generator.h5', save_weights_only=True, save_best_only=True, monitor='g_loss', mode='min')\n    checkpoint_d = ModelCheckpoint('discriminator.h5', save_weights_only=True, save_best_only=True, monitor='d_loss', mode='min')\n    \n    for epoch in range(epochs):\n        # training discriminator\n        real_data = np.reshape(dataset[np.random.randint(0, dataset.shape[0], size=batch_size)], (-1, len(features)))\n        fake_data = generator.predict(np.random.normal(0, 1, size=(batch_size, len(features))))\n        \n        x = np.concatenate([real_data, fake_data])\n        y = np.concatenate([np.ones(batch_size) * 0.9, np.zeros(batch_size) * 0.1])  # Use hashtag smoothing\n        discriminator.trainable = True\n        d_loss = discriminator.train_on_batch(x, y)\n\n        # training generator\n        noise = np.random.normal(0, 1, size=(batch_size, len(features)))\n        y_gen = np.ones(batch_size)\n        discriminator.trainable = False\n        g_loss = gan.train_on_batch(noise, y_gen)\n\n        if verbose and epoch % 100 == 0:\n            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n        if epoch % 100 == 0:  # You can adjust the frequency of saving as needed\n            generator.save_weights(generator_checkpoint_path)\n            discriminator.save_weights(discriminator_checkpoint_path)"},{"cell_type":"code","execution_count":null,"id":"8846f915-b264-42f6-a7f9-96080f784186","metadata":{"language":"python"},"outputs":[],"source":"# Prepare data\ndataset = wine_data_scaled[features].values\n\n# Training model\ntrain_gan(gan, dataset, epochs=10000)"},{"cell_type":"code","execution_count":null,"id":"ba9fbf82-ba71-4569-97d0-2d34788a4d48","metadata":{"language":"python"},"outputs":[],"source":"def generate_data(generator, n_samples=1000):\n    noise = np.random.normal(0, 1, size=(n_samples, len(features)))\n    return generator.predict(noise)\n\ndef evaluate_model(original_data, generated_data, n_clusters=10):\n    scaler = StandardScaler()\n    original_data = scaler.fit_transform(original_data)\n    generated_data = scaler.transform(generated_data)\n    \n    # Fit K-Means to the combined dataset\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(original_data)\n\n    # Predict clusters for both original and generated data\n    original_labels = kmeans.predict(original_data)\n    generated_labels = kmeans.predict(generated_data)\n\n    # Calculate the distribution of clusters for both datasets\n    original_cluster_distribution = np.bincount(original_labels, minlength=n_clusters) / len(original_labels)\n    generated_cluster_distribution = np.bincount(generated_labels, minlength=n_clusters) / len(generated_labels)\n\n    # Calculate the Wasserstein distance between the cluster distributions\n    distance = wasserstein_distance(original_cluster_distribution, generated_cluster_distribution)\n\n    # Evaluate cluster centroids for diversity\n    centroids = kmeans.cluster_centers_\n    centroid_distances = np.linalg.norm(centroids[:, np.newaxis] - centroids, axis=2)\n    diversity_score = np.mean(centroid_distances)\n\n    # Print evaluation results\n    print(f'Wasserstein distance between cluster distributions: {distance}')\n    print(f'Average distance between cluster centroids (diversity score): {diversity_score}')\n    \n    return distance, diversity_score\n\nnew_data = generate_data(generator, n_samples=1000)\nprint(new_data)\n\ngenerated_df = pd.DataFrame(new_data, columns=features)\noriginal_df = pd.DataFrame(wine_data, columns=features)\n\nif 'quality' in generated_df.columns:\n    generated_df['quality'] = np.round(generated_df['quality']).astype(int)\noriginal_data = original_df[features].values.astype(np.float32)\ngenerated_data = generated_df.values.astype(np.float32)\ndistance, diversity_score = evaluate_model(original_data, generated_data)\nprint(f'Wasserstein distance between cluster distributions: {distance}, Average distance between cluster centroids (diversity score): {diversity_score}')"},{"cell_type":"code","execution_count":null,"id":"9e420417-2227-4ba8-96ec-c95ce205e0b4","metadata":{"language":"python"},"outputs":[],"source":"# Function to find outliers with an increased multiplier for less sensitivity\ndef find_outliers(df, feature, multiplier=5):\n    Q1 = df[feature].quantile(0.25)\n    Q3 = df[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    return (df[feature] < (Q1 - multiplier * IQR)) | (df[feature] > (Q3 + multiplier * IQR))\n\n# Customize the figure size to make the plot larger\nfig_size = 40\n\n# Define colors for normal points and outliers\nnormal_color = \"#771cd6\"\noutlier_color = \"#e37d3d\"\n\n# Plot for original_df\nplt.figure(figsize=(fig_size, fig_size))\nfor i in range(len(features)):\n    for j in range(len(features)):\n        ax = plt.subplot(len(features), len(features), i * len(features) + j + 1)\n        if i == j:\n            sns.kdeplot(original_df[features[i]], fill=True, color=normal_color)\n        else:\n            sns.scatterplot(x=features[j], y=features[i], data=original_df, color=normal_color, alpha=0.6, edgecolor=None, s=30)\n        ax.xaxis.set_major_locator(ticker.AutoLocator())\n        ax.yaxis.set_major_locator(ticker.AutoLocator())\n        ax.tick_params(axis='x', labelrotation=90, labelsize=10)\n        ax.tick_params(axis='y', labelsize=10)\n        if j > 0:\n            ax.set_ylabel('')\n        if i < len(features) - 1:\n            ax.set_xlabel('')\n        if i == 0:\n            ax.set_title(features[j], fontsize=16)\n        if j == 0:\n            ax.set_ylabel(features[i], fontsize=16)\nplt.suptitle('Pairplot of the Original Data', y=1.02, size=20)\nplt.tight_layout()\nplt.show()\n\n# Define two sets to store the indexes of the exception samples\nall_outliers_indices = set()\nquality_outliers_indices = set()\n\n# Plot for generated_df\nplt.figure(figsize=(fig_size, fig_size))\nfor i in range(len(features)):\n    for j in range(len(features)):\n        ax = plt.subplot(len(features), len(features), i * len(features) + j + 1)\n        if i == j:\n            sns.kdeplot(generated_df[features[i]], fill=True, color=normal_color)\n        else:\n            sns.scatterplot(x=features[j], y=features[i], data=generated_df, color=normal_color, alpha=0.6, edgecolor=None, s=30)\n            outliers_x = find_outliers(generated_df, features[j], multiplier=5)\n            outliers_y = find_outliers(generated_df, features[i], multiplier=5)\n            sns.scatterplot(x=features[j], y=features[i], data=generated_df[outliers_x | outliers_y], color=outlier_color, alpha=0.6, edgecolor=None, s=30)\n        \n        # Update the set of all outliers indexes\n            all_outliers_indices.update(generated_df[outliers_x | outliers_y].index)\n\n            # Update the quality outlier index set only if the outlier has a quality feature >= 6\n            if 'quality' in features:\n                quality_outliers_indices.update(\n                    generated_df[(outliers_x | outliers_y) & (generated_df['quality'] >= 6)].index\n                )\n        \n        ax.xaxis.set_major_locator(ticker.AutoLocator())\n        ax.yaxis.set_major_locator(ticker.AutoLocator())\n        ax.tick_params(axis='x', labelrotation=90, labelsize=10)\n        ax.tick_params(axis='y', labelsize=10)\n        if j > 0:\n            ax.set_ylabel('')\n        if i < len(features) - 1:\n            ax.set_xlabel('')\n        if i == 0:\n            ax.set_title(features[j], fontsize=16)\n        if j == 0:\n            ax.set_ylabel(features[i], fontsize=16)"},{"cell_type":"code","execution_count":null,"id":"8daba6c1-e0b6-414d-a48c-78d9cf553d2b","metadata":{"language":"python"},"outputs":[],"source":"# Extract anomaly samples from the generated DataFrame using the collected indexes\nall_outliers_df = generated_df.iloc[list(all_outliers_indices)]\nquality_outliers_df = generated_df.iloc[list(quality_outliers_indices)]\n\n# Show samples with all outliers and outliers with QUALITY >= 6\nprint(\"All Outliers:\")\nprint(all_outliers_df)\nprint(\"\\nQuality Outliers (Quality >= 6):\")\nprint(quality_outliers_df)"},{"cell_type":"code","execution_count":null,"id":"e46ec19a-6c88-4fef-b015-a8ad414ceda3","metadata":{"language":"python"},"outputs":[],"source":"# Heatmap - shows correlation between features\nplt.figure(figsize=(10, 8))\nsns.heatmap(original_df.corr(), annot=True, fmt=\".2f\")\nplt.title('Heatmap of Correlation - Original Data')\nplt.show()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(generated_df.corr(), annot=True, fmt=\".2f\")\nplt.title('Heatmap of Correlation - Generated Data')\nplt.show()"},{"cell_type":"code","execution_count":null,"id":"4cd91497-0a22-4093-9832-da077c719103","metadata":{"language":"python"},"outputs":[],"source":"if 'Quality_Range' not in generated_df.columns:\n    generated_df['Quality_Range'] = pd.cut(generated_df['quality'], bins=[-5, 0, 2, 6], labels=['-2-0', '0-2', '2-4'])\ngenerated_df['Quality_Range'] = pd.cut(generated_df['quality'], bins=[-5, 0, 2, 6], labels=['-2-0', '0-2', '2-4'])\n\nnum_cols = generated_df.select_dtypes(include=[np.number]).columns.tolist()\nplt.figure(figsize=(14,len(num_cols)*3))\nfor idx,column in enumerate(num_cols):\n    plt.subplot(len(num_cols)//2+1,2,idx+1)\n    sns.boxplot(x=\"Quality_Range\", y=column, data=generated_df,palette=\"twilight_shifted\",legend=False)\n    plt.title(f\"{column} Distribution\")\n    plt.tight_layout()"},{"cell_type":"code","execution_count":null,"id":"9a9210eb-3e8c-443c-938b-6561b0bd8b76","metadata":{"language":"python"},"outputs":[],"source":"color_palette = ['blue', 'orange', 'green']\n# Histogram for each numerical column\nplt.figure(figsize=(14, len(num_cols) * 3))\nfor idx, column in enumerate(num_cols):\n    plt.subplot(len(num_cols) // 2 + 1, 2, idx + 1)\n    # Group data by 'Quality_Range' before plotting\n    for range_idx, range_label in enumerate(['-2-0', '0-2', '2-4']):\n        subset = generated_df[generated_df['Quality_Range'] == range_label]\n        sns.histplot(subset[column], bins=30, kde=True, color=color_palette[range_idx], label=range_label)\n    plt.legend(title='Quality Range')\n    plt.title(f\"{column} Distribution by Quality Range\")\n    plt.tight_layout()\nplt.show()"},{"cell_type":"code","execution_count":null,"id":"94aa9fb2-8446-4283-a919-4652dbb7d769","metadata":{"language":"python"},"outputs":[],"source":"def plot_count(generated_df: pd.DataFrame, col: str, title_name: str='Target Variable Distribution') -> None:\n    # Set background color\n    plt.rcParams['axes.facecolor'] = 'white'\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 8))\n    plt.subplots_adjust(wspace=0.2)\n\n    s1 = generated_df[col].value_counts()\n    N = len(s1)\n\n    outer_sizes = s1\n    inner_sizes = s1/N\n\n    outer_colors = ['#525250', '#C68C2A', '#4A291E', '#269FAC', '#FF9074'][:N]\n    inner_colors = ['#4E8278', '#DFBF87', '#9C1027', '#98BDB1', '#FECA8B'][:N]\n\n    ax[0].pie(\n        outer_sizes, colors=outer_colors, \n        labels=s1.index.tolist(), \n        startangle=90, frame=True, radius=1.3, \n        explode=([0.05]*(N-1) + [0.1]),\n        wedgeprops={'linewidth' : 1, 'edgecolor' : 'white'}, \n        textprops={'fontsize': 12, 'weight': 'bold'}\n    )\n\n    textprops = {\n        'size': 17, \n        'weight': 'bold', \n        'color': 'black'\n    }\n\n    ax[0].pie(\n        inner_sizes, colors=inner_colors,\n        radius=1, startangle=90,\n        autopct='%1.f%%', explode=([0.1]*(N-1) + [0.2]),\n        pctdistance=0.8, textprops=textprops\n    )\n\n    center_circle = plt.Circle((0,0), 0.68, color='black', fc='white', linewidth=0)\n    ax[0].add_artist(center_circle)\n\n    x = s1\n    y = s1.index.tolist()\n    sns.barplot(\n        x=x, y=y, ax=ax[1],\n        palette='pastel', orient='horizontal'\n    )\n\n    ax[1].spines['top'].set_visible(False)\n    ax[1].spines['right'].set_visible(False)\n    ax[1].tick_params(\n        axis='x',         \n        which='both',      \n        bottom=False,      \n        labelbottom=False\n    )\n\n    for i, v in enumerate(s1):\n        ax[1].text(v, i, \" \" + str(v), color='black', fontweight='bold', fontsize=15)\n\n    plt.setp(ax[1].get_yticklabels(), fontweight=\"bold\")\n    plt.setp(ax[1].get_xticklabels(), fontweight=\"bold\")\n    ax[1].set_xlabel(col, fontweight=\"bold\", color='black')\n    ax[1].set_ylabel('count', fontweight=\"bold\", color='black')\n\n    f.suptitle(title_name, fontsize=18, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n# Plot the count of 'Quality_Range'\nplot_count(generated_df, 'Quality_Range', 'Target Variable Distribution')"}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"e30bb975-06a8-479d-a9ee-be2b978a7250","defaultDatabase":"database_6a6ff"}},"nbformat":4,"nbformat_minor":5}